= Cloudera Technical Workshop

'''

Version : 1.0.0 `25th April 2023` +

'''

image:images/step0/bannerturkey.PNG[]  +

This document guides students through the Hands-on lab on of Cloudera Data Flow.
It will take you step by step to completing the pre-requisites and deliver this demo.

== Recording

The entire lab is recorded and you can watch the same to have a better understanding of the lab. +
https://fillin here[End-to-End Workshop (Recording)]. +

== Pre-Requisite (Taken Care By Instructor)

For the ease of carrying out the workshop and considering the time at hand, we have already taken care of some of the steps that need to be considered before we can start with the actual workshop steps. The prerequisites that need to be in place are: +
(1) Flow Management Data Hub Cluster should be created and running. +
(2) Streams Messaging Data Hub Cluster should be created and running. +
(3) Stream analytics Data Hub cluster should be created and running. +
(4) Data provider should be configured in SQL Stream Builder. +
(5) Have access to the file syslog-to-kafka.json. +
(6) Environment should be enabled as part of the CDF Data Service. +

Step 0 basically talks about verifying different aspects w.r.t to access and connections before we could begin with the actual steps.


== Step 0: Introduction & Set Up

=== Step 0.1: Access Verification

Your instructor will guide you through this. +
(1) https://docs.google.com/spreadsheets/d/1s63G-iBtgS8tDZOD1ml8Gh0YdunL4MtNqNzyN7E7gaY/edit#gid=412559706[Credentials]: Participants must enter their `First Name`, `Last Name` & `Company` details and make a note of corresponding `Workshop Login Username`, `Workshop Login Password` and `CDP Workload User` to be used in this workshop. +
(2) http://3.109.161.118/auth/realms/workshop/protocol/saml/clients/samlclient[Workshop login]: Using the details in the previous step make sure you are able to login here. +


=== Step 0.2: Verify permissions in Apache Ranger

*`NOTE: THESE STEPS HAVE ALREADY BEEN DONE FOR YOU, THIS SECTION WILL WALK YOU THROUGH HOW PERMISSIONS/POLICIES ARE MANAGED IN RANGER. PLEASE DO NOT EXECUTE THE STEPS IN THIS SECTION OR CHANGE ANYTHING.`*

(1) Accessing Apache Ranger +

(2) Kafka Permissions +

(3) Schema Registry Permissions +


== Step 1: Define Workload Password

Please use the login url: http://3.109.161.118/auth/realms/workshop/protocol/saml/clients/samlclient[Workshop login]. +
Enter the `Workshop Login Username` and `Workshop Login Password` that you obtained as part of `Step 0.1`. +
(*Note*: Note that your Workshop Login Username would be something like `wuser00@workshop.com` and not just `wuser00`). +

image::images/step1/1.PNG[] +

You should be able to get the following home page of CDP Public Cloud. +

image::images/step2/2.PNG[] +

You will need to define your workload password that will be used to acess non-SSO interfaces. You may read more about it here (https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-access-paths-to-cdp.html).
Please keep it with you. If you have forgotten it, you will be able to repeat this process and define another one.

. Click on your `user name (Ex: wuser00@workshop.com`) at the lower left corner.
. Click on the `Profile` option.

image:images/step4/1.PNG[]  +

. Click option `Set Workload Password`.
. Enter a suitable `Password` and `Confirm Password`.
. Click button `Set Workload Password`.


image:images/step4/2.PNG[]  +

image::images/step4/3.PNG[]

{blank} +

Check that you got the message - `Workload password is currently set` or alternatively, look for a message next to `Workload Password` which says `(Workload password is currently set)`

image::images/step4/4.PNG[]


== Overview

In this exercise, we will work get stock data from https://www.alphavantage.co/[Alpha Vantage], that offers free stock APIs in JSON and CSV formats for real-time and historical stock market data.

* Data ingestion and streaming -- provided by *_Cloudera Data Flow (CDF)_* and *_Cloudera Data Engineering (CDE)_*.
* Global data access and persistence--provided by *_Cloudera Data Warehouse (CDW)_*.
* Data visualization with *_CDP Data Visualization_*.

*_Cloudera DataFlow (CDF)_* offers a flow-based low-code development paradigm that aligns best with how developers design, develop, and test data distribution pipelines. With over 450+ connectors and processors across the ecosystem of hybrid cloud services—including data lakes, lakehouses, cloud warehouses, and on-premises sources—CDF-PC provides indiscriminate data distribution. These data distribution flows can then be version-controlled into a catalog where operators can self-serve deployments to different runtimes.

*_Cloudera Data Engineering (CDE)_* is the only cloud-native service purpose-built for enterprise data engineering teams. Building on Apache Spark, Data Engineering is an all-inclusive data engineering toolset that enables orchestration automation with Apache Airflow, advanced pipeline monitoring, visual troubleshooting, and comprehensive management tools to streamline ETL processes across enterprise analytics teams.

*_Cloudera Data Warehouse (CDW)_* is a cloud service for creating self-service data warehouses and the underlying compute clusters for teams of business analysts. Data Warehouse is an auto-scaling, highly concurrent and cost effective analytics service that ingests high scale data anywhere, from structured, unstructured and edge sources. It supports hybrid and multi-cloud infrastructure models by seamlessly moving workloads between on-premise and any cloud for reports, dashboards, ad-hoc and advanced analytics, including AI, with consistent security and governance.

*_CDP Data Visualization_* enables data engineers, business analysts, and data scientists explore data quickly and easily, collaborate, and share insights across the data lifecycle--from data ingest to data insights and beyond. Delivered natively as part of Cloudera Data Platform (CDP), Data Visualization delivers a consistent and easy to use data visualization experience with intuitive and accessible drag-and-drop dashboards and custom application creation.

image:images/step0/architecture.PNG[]  +

== High-Level Steps

Below are the high-level steps for what we will be doing in the workshop. +
(1) Get Alpha Vantage key to be used in Cloudera Data Flow (CDF) to collect stock data (IBM, AMZN, MSFT, GOOGL). +
(2) Create CDF workflow and run it to ingest data into S3. +
(3) Create Iceberg Table using Cloudera Data Warehouse (CDW/Hue). +
(4) Create CDE job and run it to ingest data into iceberg table. +
(5) Use Cloudera Data Viz to create a simple dashboard on Iceberg table. +
(6) Run the CDE job with updated ticker (NVDA). +
(7) Use/Test Iceberg time travel features. +


== Pre-requisites

. Laptop with a supported OS (Windows 7 not supported) or MacBook.
. A modern browser - Google Chrome (IE, Firefox, Safari not supported).
. Wi-Fi Internet connection.
. Git installed.

Please complete only one of the two following steps: `Step 1(a)` or `Step 1(b)`. Follow `Step 1(a)` if you have Git installed on your machine, else, follow `Step 1(b)`. +

== Step 0: Access Details

Your instructor will guide you through this. +
(1) https://docs.google.com/spreadsheets/d/1s63G-iBtgS8tDZOD1ml8Gh0YdunL4MtNqNzyN7E7gaY/edit#gid=412559706[Credentials]: Participants must enter their `First Name`, `Last Name` & `Company` details and make a note of corresponding `Workshop Login Username`, `Workshop Login Password` and `CDP Workload User` to be used in this workshop. +
(2) http://3.109.161.118/auth/realms/workshop/protocol/saml/clients/samlclient[Workshop login]: Using the details in the previous step make sure you are able to login here. +

== Step 1(a): Get github project

{blank}

You can use the workshop project cloning this github repository : https://github.com/DashDipti/partner-summit-2023 [Workshop Github repo]

[,console]
----
git clone https://github.com/DashDipti/partner-summit-2023
----

== Step 1(b): Download repository using GUI

{blank}
Scroll up the page here (https://github.com/DashDipti/partner-summit-2023) and click on `<> Code` and then choose the option `Download ZIP`. +
image:images/step1/1.PNG[] +

Use any unzip utility to download extract the content of the `partner-summit-2023-main.zip` file. +
image:images/step1/1.PNG[] +

In the extracted content just be sure that the downloaded files has a file *`Stocks_Intraday_Alpha_Template.json`* which should be around *~65* KB in size. You will need this file in later step. +
image:images/step1/1.PNG[] +


== Step 2: Get Alpha Vantage Key

. Go to website https://www.alphavantage.co/[Alpha Vantage].
. Choose link \-> `GET YOUR FREE API KEY TODAY`.

image:images/step2/alphaVantagePortal.PNG[]

. Choose 'Student' for the question - `Which of the following best describes you?`.
. Enter your own organisation name for the question - `Organization (e.g. company, university, etc.):`
. Enter your email address for the question - `Email:` (Note: Please enter personal email id and not the workshop email id)
. Click on `GET FREE API KEY`.

image:images/step2/claimApiKey.PNG[]


You should see a message like - 'Welcome to Alpha Vantage! Your dedicated access key is: YXXXXXXXXXXXXXXE. 
`*Please record this API key at a safe place for future data access.*`

image:images/step2/getKey.PNG[] +

== Step 3: Access CDP Public Cloud Portal

Please use the login url: http://3.109.161.118/auth/realms/workshop/protocol/saml/clients/samlclient[Workshop login]. +
Enter the `Workshop Login Username` and `Workshop Login Password` that you obtained as part of `Step 0`. +
(*Note*: Note that your Workshop Login Username would be something like `wuser00@workshop.com` and not just `wuser00`).

image::images/step3/1.PNG[]

You should be able to get the following home page of CDP Public Cloud.

image::images/step3/2.PNG[]

== Step 4: Define Workload Password

You will need to define your workload password that will be used to acess non-SSO interfaces. You may read more about it here (https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-access-paths-to-cdp.html).
Please keep it with you. If you have forgotten it, you will be able to repeat this process and define another one.

. Click on your `user name (Ex: wuser00@workshop.com`) at the lower left corner.
. Click on the `Profile` option.

image:images/step4/1.PNG[]  +

. Click option `Set Workload Password`.
. Enter a suitable `Password` and `Confirm Password`.
. Click button `Set Workload Password`.


image:images/step4/2.PNG[]  +

image::images/step4/3.PNG[]

{blank} +

Check that you got the message - `Workload password is currently set` or alternatively, look for a message next to `Workload Password` which says `(Workload password is currently set)`

image::images/step4/4.PNG[]



== Step 5: Create the flow to ingest stock data via API to Object Storage

=== CDP Portal



Click on `Home` option on top left corner to go to the landing page.

image:images/step5/1.PNG[]  +

Click on `DataFlow` icon as shown in the image below.

image:images/step5/2.PNG[]  +


=== Create a new CDF Catalog

. On the left menu click on the option \-> `Catalog`. +
. On the top right corner click the button \-> `Import Flow Definition`.


image:images/step5/3.PNG[]  +

Fill up those parameters : +

`Flow Name` +

____
(user)-stock-data +
____

Depending upon your user name it should be something like - `wuser00-stock-data`. +

`Nifi Flow Configuration`
____
Upload the file *Stocks_Intraday_Alpha_Template.json* +
(*`Note`*: You had downloaded this file in `Step 1(a)` or `Step 1(b)` depending on what you chose initially.).
____

Click `Import` +

image:images/step5/4.PNG[]  +

The new catalog has been added. Type in the name so that you can only see the one that you had created and not the others. For example - `wuser00-stock-data` +

image:images/step5/5.PNG[]  +

Now let's deploy it.

=== Deploy DataFlow

Click on the small arrow towards right of the catalog you just created. Click on `Deploy` button.

image:images/step5/6.PNG[]  +
You will need to select the workshop environment *`meta-workshop`*. +
Click on `Continue ->`

image:images/step5/7.PNG[]  +
Give a name to this dataflow. +
`Deployment Name`

____
(user)_stock_dataflow +
____
Depending on your user name it should be something like - `wuser00_stock_dataflow`. +

Make sure that the right `Target Environment` is selected.
Click `Next`.

image:images/step5/8.PNG[]  +

Let parameters be the default ones. Click `Next`.


image:images/step5/9.PNG[]  +

`CDP_Password` +

____
Fill up your CDP workload password here +
____

`CDP_User` +

____
your user +
____
Depending on your user name it should be something like - `wuser00`. +

`S3 Path` +

____
stocks +
____

`api_alpha_key` +

____
your Alpha Vantage key +
____

`stock_list` +

____
IBM +
GOOGL +
AMZN +
MSFT
____

Click `Next ->`.

image:images/step5/10.PNG[]  +
`Nifi Node Sizing` +

____
Extra Small +
____

Slide button to right to `Enable Auto scaling` and let the min nodes be 1 and max nodes be 3. +

____
Let parameters by default +
____

Click `Next->`.

image:images/step5/11.PNG[]  +

You can define KPI's in regards what has been specified in your dataflow, but we will skip this for now.
Click `Next->` +

image:images/step5/12.PNG[]  +

Click `Deploy` to launch the deployment. +

image:images/step5/13.PNG[]  +

The deployment will get initiated. Check the deployment on the run and look for the status `Good Health`.

image:images/step5/14.PNG[]  +

image:images/step5/15.PNG[]  +

Dataflow is up and running and you can confirm the same by looking at the green tick and message `Good Health` against the dataflow name. It's okay to wait for 2-3 minutes here before you see the green tick. Notice the `Event History` and there are approximately 8 steps that happen after the flow deployment. You might want to observe those.

image:images/step5/15-1.PNG[]  +
image:images/step5/16.PNG[]  +

In minutes we will start receiving stock information into our bucket.
If you want you can check in your bucket under the path `s3a://meta-workshop/user/(username)/stocks/new`. +
*Note*: You don't have access to the S3 bucket. The instructor will confirm if the data files have been received after your workflow runs. +
*Let the instructor know if you have reached this section.*


=== View Nifi DataFlow

Click on blue arrow on the right of your deployed dataflow `wuser00_stock_dataflow`.

image:images/step5/16.PNG[]  +

Select `Manage Deployment` on top right corner.

image:images/step5/17.PNG[]  +

On this window, choose `Actions` \-> `View in NiFi`.

image:images/step5/18.PNG[]  +

image:images/step5/19.PNG[]  +


You can see the Nifi data flow that has been deployed from the json file.
Let's take a quick look together.

image:images/step5/20.PNG[]  +


At this stage you can suspend this dataflow, go back to `Deployment Manager` \-> `Actions` \-> `Suspend flow`.
We will add a new stock later and restart it.

image:images/step5/21.PNG[]  +

On getting the pop up, click on `Suspend Flow`.

image:images/step5/22.PNG[]  +

Confirm that the status is `Suspended`.

image:images/step5/23.PNG[]  +

== Step 6: Create Iceberg Table

Now we are going to create the Iceberg table.
Click on `Home` option on top left corner to go to the landing page.

image:images/step6/1.PNG[]  +

From the CDP Portal or CDP Menu choose `Data Warehouse`.

image:images/step6/2.PNG[]  +

From the CDW `Overview` window, click the "HUE" button on the right corner as shown under the `Virtual Warehouses` to the right.

image:images/step6/3.PNG[]  +

Now you're accessing to the sql editor called "HUE" (Hadoop User Experience).

image:images/step6/4.PNG[]  +

Let's select the Impala engine that you will be using for interacting with database.
On the top left corner select `</>` and select the Editor to be `Impala`.

Make sure that you can see `Impala` instead of `Unified Analytics` on top of the area where you would write queries.

image:images/step6/5.PNG[]  +

Create database using your login `For example: wuser00`. Replace `<user>` by your username for database creation in the command below.

[,sql]
----

CREATE DATABASE <user>_stocks;
----

See the result to notice a message `Database has been created`.

image:images/step6/6.PNG[]  +

After creating the database create an Iceberg table. Replace `<user>` by your username for iceberg table creation in the command below.

[,sql]
----

CREATE TABLE IF NOT EXISTS <user>_stocks.stock_intraday_1min (
  interv STRING,
  output_size STRING,
  time_zone STRING,
  open DECIMAL(8,4),
  high DECIMAL(8,4),
  low DECIMAL(8,4),
  close DECIMAL(8,4),
  volume BIGINT)
PARTITIONED BY (
  ticker STRING,
  last_refreshed string,
  refreshed_at string)
STORED AS iceberg;
----

See the result to notice a message `Table has been created`.

image:images/step6/7.PNG[]  +

Let's now create our engineering process.


== Step 7: Process and Ingest Iceberg using CDE

Now we will use Cloudera Data Engineering to check the files in the object storage that were populated as a part of the above DataFlow run and then compare if it's new data, and insert them into the Iceberg table.

Click on `Home` option on top left corner to go to the landing page.

image:images/step7/1.PNG[]  +

From the CDP Portal or CDP Menu choose `Data Engineering`.

image:images/step7/2.PNG[]  +

Let's create a job. 
Click on `Jobs`. Make sure that you can see `meta-workshop-de` on the top.  +
Then click `Create Job` button in the right side of the screen. +
*Note*: This page may differ a little bit depending on the fact that some user may have created a job prior to you or not.

image:images/step7/3.PNG[]  +

*`Fill the following values carefully`*.

`Job Type*`

____
Choose Spark 3.2.0
____

`Name*` +
Replace `(user)` with your username. For example: `wuser00-StockIceberg`. +

____
(user)-StockIceberg
____

Make sure `Application File` that is selected is `File`. Select the option `Select from Resource`.

____
Select  stockdata-job \-> stockdatabase_2.12-1.0.jar
____

image:images/step7/4.PNG[]  +

`Main Class`

____
com.cloudera.cde.stocks.StockProcessIceberg
____

Make sure the below arguments are filled so that (user) is replaced with the actual username. For example `wuser00_stocks` and instead of (user) at the end it is `wuser00`. Make sure to check the next screenshot to comply.

`Arguments`

____
(user)_stocks +
s3a://meta-workshop/ +
stocks +
(user) +
____


image:images/step7/5.PNG[]  +

Click the `Create and Run` button at the bottom. (There is no screenshot for the same). +
*Note*: It might take ~3 minutes. So, it's okay to wait until it's done.


This application will:

* Check new files in the new directory;
* Create a temp table in Spark/cache this table and identify duplicated rows (in case that NiFi loaded the same data again);
* MERGE INTO the final table, INSERT new data or UPDATE if exists;
* Archive files in the bucket;

After execution, the processed files will be in your bucket but under the name which has the format - `processed"+date/`. 

image:images/step7/6.PNG[]  +

You don't have access to it. The instructor has access to the same. The next section is optional.

== Step 7 (Optional): Checking Logs of CDE Job Run
Click on the Job Name - `wuser-StockIceberg`.
image:images/step7/7.PNG[]  +

Click on the `Run Id`.
image:images/step7/8.PNG[]  +

You will reach the `Trends` option.
image:images/step7/9.PNG[]  +

Click the `Logs` and go through the various tabs like 'stderr+stdout' to understand better.
image:images/step7/10.PNG[]  +

Under `Logs` tab check for the following. In most of the cases `Processing temp dirs` indicates that job would run successfully and is in it's last stages.
image:images/step7/11.PNG[]


== Step 8: Create Dashboard using CDP DataViz

*Note*: Before moving ahead with this section make sure that the *CDE job ran successfully*. Go to `Job Runs` option in the left pane and look for the job that you ran now. It should have a green tick box next to it's name.

image:images/step8/1.PNG[] +

We will now create a simple dashboard using Cloudera Data Viz.

Click on `Home` option on top left corner to go to the landing page.

image:images/step8/2.PNG[]  +

From the CDP Portal or CDP Menu choose `Data Warehouse`.

image:images/step8/3.PNG[]  +

You will reach the `Overview` page. 

image:images/step8/4.PNG[]  +

In the menu on the left choose `Data Visualization`.
Look for `meta-workshop-dataviz`. Then click the `Data VIZ` button on the right. 

image:images/step8/5.PNG[]  +

You will access to the following window. Choose `DATA` on the upper menu bar next to the options of HOME, SQL, VISUALS. +
image:images/step8/6.PNG[]  +

Click `meta-workshop` option in the left pane and then click on `NEW DATASET` option on top.

image:images/step8/7.PNG[]  +

Replace `(user)` with your username wherever it is applicable. +
`Dataset title` +

____
(user)_dataset +
____

`Dataset Source` +

____
From Table +
____

`Select Database` +

____
(user)_stocks
____

`Select Table` +

____
stock_intraday_1min
____

Click `CREATE`.

image:images/step8/8.PNG[]  +

Select "New Dashboard" \-> image:images/step8/9.PNG[] icon next to the Table that you created just now.

image:images/step8/10.PNG[]

You'll land in the following page.
image:images/step8/11.PNG[]

Let's drag from `DATA` section on the right under `Dashboard Designer` the following attribute/metric. And the 'REFRESH THE VISUAL'

`Dimensions` \-> `ticker` +

____
Move it to Visuals \-> `Dimensions`
____

`Measures` \-> `#volume` +

____
Move it to Visuals \-> `Measures`
____

image:images/step8/12.PNG[]

Then on 'VISUALS' choose `Packed Bubbles`. +

image:images/step8/13.PNG[]
Your visual could be slighltly different from the image here.

Make it PUBLIC by changing the option from `PRIVATE` to `PUBLIC`. Save it by clicking the `SAVE` button on the top.  You have succeeded to create a simple dashboard. Now, let's query our data and explore the time-travel and snapshot capabilties of Iceberg.

== Step 9: Query Iceberg Tables in Hue and Cloudera Data Visualization

=== Step 9(a): For Reading only (Optional): Iceberg Architecture

Apache Iceberg is an open table format, originally designed at Netflix to overcome the challenges faced when using already existing data lake formats like Apache Hive.

The design structure of Apache Iceberg is different from Apache Hive, where the metadata layer and data layer are managed and maintained on object storage like Hadoop, s3, etc.

It uses a file structure (metadata and manifest files) that is managed in the metadata layer.
Each commit at any timeline is stored as an event on the data layer when data is added.
The metadata layer manages the snapshot list.
Additionally, it supports integration with multiple query engines,

Any update or delete to the data layer, creates a new snapshot in the metadata layer from the previous latest snapshot and parallelly chains up the snapshot, enabling faster query processing as the query provided by users pulls data at the file level rather than at the partition level.

{blank} +

image:images/step0/iceberg-architecture.PNG[] +

Our example will load the intraday stock daily since the free API does not give real-time data, but we can change the Cloudera Dataflow Parameter to add one more ticker and we've scheduled to run hourly the CDE process.
After this we will be able to see the new ticker information in the dashboard and also *perform time travel using Iceberg!*

=== Step 9(b): Logging into Hue
Click on `Home` option on top left corner to go to the landing page.

image:images/step9/1.PNG[]  +

From the CDP Portal or CDP Menu choose `Data Warehouse`.

image:images/step9/2.PNG[]  +

From the CDW `Overview` window, click the "HUE" button on the right corner as shown under the `Virtual Warehouses` to the right. Make sure that the correct 'Virtual Warehouse' is selected - In this case it is `meta-workshop-ww`.

image:images/step9/3.PNG[]  +

Now you're accessing to the sql editor called "HUE".

image:images/step9/4.PNG[]  +

Let's select the Impala engine that you will be using for interacting with database.
On the top left corner select `</>` and select the Editor to be `Impala`.

Make sure that you can see `Impala` instead of `Unified Analytics` on top of the area where you would write queries.

image:images/step9/5.PNG[]  +


=== Step 9(c): Iceberg snapshots

Let's see the Iceberg table history.
Replace <user> with your username. For example: `wuser00`.

[,sql]
----

DESCRIBE HISTORY <user>_stocks.stock_intraday_1min;
----

{blank} +

image:images/step9/6.PNG[]  +

{blank} +

Copy and paste the `snapshot_id` and use it on the following impala queries. Replace <user> with your username. For example: `wuser00`.

[,sql]
----

SELECT ticker, count(*)
FROM <user>_stocks.stock_intraday_1min
FOR SYSTEM_VERSION AS OF <snapshot_id>
GROUP BY ticker;
----

{blank} +

image:images/step9/7.PNG[]  +

{blank} +

=== Step 9(d): Add a New stock (NVDA)

We shall load new data and this time we will include additional stock ticker - `NVDA`.
Go to CDF, and find the data flow that you had created earlier. It should be in suspended state if you had suspended it towards the end of +
`Step 5: Create the flow to ingest stock data via API to Object Storage` section of the workshop.

Go to Cloudera Data Flow option and look for the flow that you had created earlier based on your user name. Ex - `wuser00_stock_dataflow`. Click on the arrow towards the right side of the flow and then click on `Manage Deployment`.

image:images/step9/8.PNG[]  +

image:images/step9/9.PNG[]  +

Click on the `Parameters` tab and then scroll down to the text box where you had entered stock tickers (`stock_list`). 

image:images/step9/10.PNG[]  +

Add the stock 'NVDA'. And then click on `Apply Changes`.
image:images/step9/11.PNG[]  +
image:images/step9/12.PNG[]  +

Now, start the flow again by clicking `Actions` and then `Start flow`.
image:images/step9/13.PNG[]  +
image:images/step9/14.PNG[]  +
image:images/step9/15.PNG[]  +

The S3 bucket gets updated with new data and this time it includes the new ticker 'NVDA' as well. We will see it. You can see the same in S3 bucket as shown here.
image:images/step9/16.PNG[]  +

Now go to Cloudera `Data Engineering` from the home page and `Jobs`. Choose the CDE Job that you had created earlier with your username.
image:images/step9/17.PNG[]  +


Click the 3 dots next to your job that you had created earloer and then click on `Run Now`.
image:images/step9/18.PNG[]  +
image:images/step9/19.PNG[]  +

Click on `Job Runs` in the left to see the status of the job that was initiated now. It should succeed.
image:images/step9/20.PNG[]  +
image:images/step9/21.PNG[]  +

{blank} +

As CDF has ingested a new stock value and then CDE has merged those value it has created new Iceberg snapshots. Copy and paste the new 'snapshot_id' and use it on the following impala query.

=== Step 9(e): Check new snapshot history

Now let check again the snapshot history by going to Hue.

[,sql]
----

DESCRIBE HISTORY <user>_stocks.stock_intraday_1min;
----

{blank} +

image:images/step9/22.PNG[]  +

[,sql]
----

SELECT ticker, count(*)
FROM <user>_stocks.stock_intraday_1min
FOR SYSTEM_VERSION AS OF <new_snapshot_id>
GROUP BY ticker;
----

{blank} +

image:images/step9/23.PNG[]  +

{blank} +

Now, we can see that this snapshot retrieves the count value for stock NVDA that has been added in the CDF `stock_list` parameter.

=== Show Data Files
Replace <user> with your username. For example: `wuser00`.
[,sql]
----

show files in <user>_stocks.stock_intraday_1min;
----

{blank} +

image:images/step9/24.PNG[]  +

{blank} +


Check the Iceberg table. Replace <user> with your username. For example: `wuser00`.
[,sql]
----

describe formatted <user>_stocks.stock_intraday_1min;
----

{blank} +

image:images/step9/25.PNG[]  +

{blank} +


*`Note`*: Please make sure that the data flow that was created by you is 'suspended' else it will be running continously.